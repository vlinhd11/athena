{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_tts.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DILug_IcEKfq"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import logging\n",
        "from athena.models.tts_transformer import TTSTransformer\n",
        "#from athena.solver import BaseSolver\n",
        "from athena.utils.checkpoint import Checkpoint\n",
        "from athena.utils.learning_rate import WarmUpLearningSchedule, WarmUpAdam\n",
        "from athena.utils.hparam import HParams, register_and_parse_hparams\n",
        "from athena.utils.metric_check import MetricChecker\n",
        "from athena.utils.misc import validate_seqs\n",
        "from athena.metrics import CharactorAccuracy\n",
        "from athena.data.datasets.speech_synthesis import SpeechSynthesisDatasetBuilder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwpWF5M7HEMG"
      },
      "source": [
        "!pip install kenlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVCT3NWUEOp6"
      },
      "source": [
        "import pandas as pd\n",
        "import functools\n",
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr98DVCwEQJR"
      },
      "source": [
        "DEFAULT_CONFIGS = {\n",
        "    \"batch_size\": 32,\n",
        "    \"num_epochs\": 20,\n",
        "    \"sorta_epoch\": 1,\n",
        "    \"ckpt\": None,\n",
        "    \"summary_dir\": None,\n",
        "    \"solver_type\": \"asr\",\n",
        "    \"solver_gpu\": [0],\n",
        "    \"solver_config\": None,\n",
        "    \"model\": \"speech_transformer\",\n",
        "    \"num_classes\": None,\n",
        "    \"model_config\": None,\n",
        "    \"pretrained_model\": None,\n",
        "    \"teacher_model\": None,\n",
        "    \"optimizer\": \"warmup_adam\",\n",
        "    \"optimizer_config\": None,\n",
        "    \"convert_config\": None,\n",
        "    \"num_data_threads\": 1,\n",
        "    \"dataset_builder\": \"speech_recognition_dataset\",\n",
        "    \"dev_dataset_builder\": None,\n",
        "    \"trainset_config\": None,\n",
        "    \"devset_config\": None,\n",
        "    \"testset_config\": None,\n",
        "    \"inference_config\": None\n",
        "}\n",
        "Data_default_config = {\n",
        "        \"audio_config\": {\"type\": \"Fbank\"},\n",
        "        \"text_config\": {\"type\":\"vocab\", \"model\":\"athena/utils/vocabs/ch-en.vocab\"},\n",
        "        \"num_cmvn_workers\": 1,\n",
        "        \"cmvn_file\": None,\n",
        "        \"remove_unk\": True,\n",
        "        \"input_length_range\": [20, 50000],\n",
        "        \"output_length_range\": [1, 10000],\n",
        "        \"speed_permutation\": [1.0],\n",
        "        \"spectral_augmentation\": None,\n",
        "        \"data_csv\": None,\n",
        "        \"words\": None\n",
        "    }\n",
        "def parse_config(config):\n",
        "    \"\"\" parse config \"\"\"\n",
        "    p = register_and_parse_hparams(DEFAULT_CONFIGS, config, cls=\"main\")\n",
        "    logging.info(\"hparams: {}\".format(p))\n",
        "    return p\n",
        "\n",
        "def parse_jsonfile(jsonfile):\n",
        "    \"\"\" parse the jsonfile, output the parameters\n",
        "    \"\"\"\n",
        "    config = None\n",
        "    with open(jsonfile) as file:\n",
        "        config = json.load(file)\n",
        "    p = register_and_parse_hparams(DEFAULT_CONFIGS, config, cls=\"main\")\n",
        "    logging.info(\"hparams: {}\".format(p))\n",
        "    return p\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDozp03iERer"
      },
      "source": [
        "class BaseSolver(tf.keras.Model):\n",
        "    \"\"\"Base Solver.\n",
        "    \"\"\"\n",
        "    default_config = {\n",
        "        \"clip_norm\": 100.0,\n",
        "        \"log_interval\": 10,\n",
        "        \"enable_tf_function\": True\n",
        "    }\n",
        "    def __init__(self, model, optimizer, sample_signature, eval_sample_signature=None,\n",
        "                 config=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.metric_checker = MetricChecker(self.optimizer)\n",
        "        self.sample_signature = sample_signature\n",
        "        self.eval_sample_signature = eval_sample_signature\n",
        "        self.hparams = register_and_parse_hparams(self.default_config, config, cls=self.__class__)\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize_devices(solver_gpus=None):\n",
        "        \"\"\" initialize hvd devices, should be called firstly \"\"\"\n",
        "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        # means we're running in GPU mode\n",
        "        if len(gpus) != 0:\n",
        "            # If the list of solver gpus is empty, the first gpu will be used.\n",
        "            if len(solver_gpus) == 0:\n",
        "                solver_gpus.append(0)\n",
        "            assert len(gpus) >= len(solver_gpus)\n",
        "            for idx in solver_gpus:\n",
        "                tf.config.experimental.set_visible_devices(gpus[idx], \"GPU\")\n",
        "\n",
        "    @staticmethod\n",
        "    def clip_by_norm(grads, norm):\n",
        "        \"\"\" clip norm using tf.clip_by_norm \"\"\"\n",
        "        if norm <= 0:\n",
        "            return grads\n",
        "        grads = [\n",
        "            None if gradient is None else tf.clip_by_norm(gradient, norm)\n",
        "            for gradient in grads\n",
        "        ]\n",
        "        return grads\n",
        "\n",
        "    def train_step(self, samples):\n",
        "        \"\"\" train the model 1 step \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            # outputs of a forward run of model, potentially contains more than one item\n",
        "            outputs = self.model(samples, training=True)\n",
        "            loss, metrics = self.model.get_loss(outputs, samples, training=True)\n",
        "            total_loss = sum(list(loss.values())) if isinstance(loss, dict) else loss\n",
        "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        grads = self.clip_by_norm(grads, self.hparams.clip_norm)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss, metrics\n",
        "\n",
        "    def train(self, dataset, total_batches=-1):\n",
        "        \"\"\" Update the model in 1 epoch \"\"\"\n",
        "        train_step = self.train_step\n",
        "        if self.hparams.enable_tf_function:\n",
        "            print(\"please be patient, enable tf.function, it takes time ...\")\n",
        "            train_step = tf.function(train_step, input_signature=self.sample_signature)\n",
        "        for batch, samples in enumerate(dataset.take(total_batches)):\n",
        "            # train 1 step\n",
        "            samples = self.model.prepare_samples(samples)\n",
        "            loss, metrics = train_step(samples)\n",
        "            if batch % self.hparams.log_interval == 0:\n",
        "                print(self.metric_checker(loss, metrics), end='\\r')\n",
        "                self.model.reset_metrics()\n",
        "    \n",
        "    def train_and_eval(self, dataset, total_batches=-1, total_epoch=50):\n",
        "        \"\"\" Update the model in 1 epoch \"\"\"\n",
        "        train_step = self.train_step\n",
        "        if self.hparams.enable_tf_function:\n",
        "            print(\"please be patient, enable tf.function, it takes time ...\")\n",
        "            train_step = tf.function(train_step, input_signature=self.sample_signature)\n",
        "        epoch = 0\n",
        "        \n",
        "        for batch, samples in enumerate(dataset.take(total_batches)):\n",
        "            # train 1 step\n",
        "            samples = self.model.prepare_samples(samples)\n",
        "            loss, metrics = train_step(samples)\n",
        "            if batch % self.hparams.log_interval == 0:\n",
        "                print(self.metric_checker(loss, metrics), end='\\r')\n",
        "                self.model.reset_metrics()\n",
        "\n",
        "    def evaluate_step(self, samples):\n",
        "        \"\"\" evaluate the model 1 step \"\"\"\n",
        "        # outputs of a forward run of model, potentially contains more than one item\n",
        "        outputs = self.model(samples, training=False)\n",
        "        loss, metrics = self.model.get_loss(outputs, samples, training=False)\n",
        "        return loss, metrics\n",
        "\n",
        "    def evaluate(self, dataset, epoch):\n",
        "        \"\"\" evaluate the model \"\"\"\n",
        "        loss_metric = tf.keras.metrics.Mean(name=\"AverageLoss\")\n",
        "        loss, metrics = None, None\n",
        "        evaluate_step = self.evaluate_step\n",
        "        if self.hparams.enable_tf_function:\n",
        "            print(\"please be patient, enable tf.function, it takes time ...\")\n",
        "            evaluate_step = tf.function(evaluate_step, input_signature=self.eval_sample_signature)\n",
        "        self.model.reset_metrics()  # init metric.result() with 0\n",
        "        for batch, samples in enumerate(dataset):\n",
        "            samples = self.model.prepare_samples(samples)\n",
        "            loss, metrics = evaluate_step(samples)\n",
        "            if batch % self.hparams.log_interval == 0:\n",
        "                print(self.metric_checker(loss, metrics, -2), end='\\r')\n",
        "            total_loss = sum(list(loss.values())) if isinstance(loss, dict) else loss\n",
        "            loss_metric.update_state(total_loss)\n",
        "        print(self.metric_checker(loss_metric.result(), metrics, evaluate_epoch=epoch))\n",
        "        self.model.reset_metrics()\n",
        "        return loss_metric.result(), metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WIgPn92EVNv"
      },
      "source": [
        "jsonfile ='/content/drive/MyDrive/AI/DATA/Donglinh/tts_transformer.json'\n",
        "p = parse_jsonfile(jsonfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3p6FhJNEbw5",
        "outputId": "5d66355a-bbc9-4722-a83f-454b569119f8"
      },
      "source": [
        "!pip install --ignore-installed /content/drive/MyDrive/AI/DATA/Donglinh/athena/dist/athena-0.1.0*.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./drive/MyDrive/AI/DATA/Donglinh/athena/dist/athena-0.1.0-cp37-cp37m-linux_x86_64.whl\n",
            "Installing collected packages: athena\n",
            "Successfully installed athena-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzKtqs-KIq0_"
      },
      "source": [
        "testset_builder = SpeechSynthesisDatasetBuilder(p.testset_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2yfRFBpIuC_"
      },
      "source": [
        "trainset_builder = SpeechSynthesisDatasetBuilder(p.trainset_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwRCRM1RI2OQ",
        "outputId": "4dfe3284-f827-46fc-9060-c714cde37d76"
      },
      "source": [
        "trainset_builder.compute_cmvn_if_necessary(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11726/11726 [1:15:43<00:00,  2.58it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<athena.data.datasets.speech_synthesis.SpeechSynthesisDatasetBuilder at 0x7f59a4d9c3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WApg9NZeK3cK"
      },
      "source": [
        "rank = 0\n",
        "rank_size = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "572vPD5PKs-D",
        "outputId": "1976a64b-18d2-48ce-965c-f93d6bb847da"
      },
      "source": [
        "trainset_builder.shard(rank_size, rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<athena.data.datasets.speech_synthesis.SpeechSynthesisDatasetBuilder at 0x7f59a4d9c3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD_o7zW7pWA7",
        "outputId": "61874ac4-8126-4576-f46d-d05a9f78d0e1"
      },
      "source": [
        "model = TTSTransformer(\n",
        "        data_descriptions=trainset_builder,\n",
        "        config=p.model_config,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"enc\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 512)         69120     \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 512)         1310720   \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 512)         1310720   \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 512)         1310720   \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 512)         1574912   \n",
            "=================================================================\n",
            "Total params: 5,582,336\n",
            "Trainable params: 5,579,264\n",
            "Non-trainable params: 3,072\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"prenet\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, None, 240)]       0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 256)         61696     \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 256)         65792     \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, None, 256)         0         \n",
            "=================================================================\n",
            "Total params: 127,488\n",
            "Trainable params: 127,488\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"postnet\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, None, 80)]        0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, None, 512)         204800    \n",
            "_________________________________________________________________\n",
            "tf.math.tanh (TFOpLambda)    (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, None, 512)         1310720   \n",
            "_________________________________________________________________\n",
            "tf.math.tanh_1 (TFOpLambda)  (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, None, 512)         1310720   \n",
            "_________________________________________________________________\n",
            "tf.math.tanh_2 (TFOpLambda)  (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, None, 512)         1310720   \n",
            "_________________________________________________________________\n",
            "tf.math.tanh_3 (TFOpLambda)  (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, None, 512)         1310720   \n",
            "_________________________________________________________________\n",
            "tf.math.tanh_4 (TFOpLambda)  (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "projection (Dense)           (None, None, 80)          41040     \n",
            "=================================================================\n",
            "Total params: 5,498,960\n",
            "Trainable params: 5,493,840\n",
            "Non-trainable params: 5,120\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"x_net\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 512)         69120     \n",
            "_________________________________________________________________\n",
            "scaled_positional_encoding ( (None, None, 512)         1         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, None, 512)         0         \n",
            "=================================================================\n",
            "Total params: 69,121\n",
            "Trainable params: 69,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"y_net\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, None, 240)]       0         \n",
            "_________________________________________________________________\n",
            "prenet (Functional)          (None, None, 256)         127488    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, None, 512)         131584    \n",
            "_________________________________________________________________\n",
            "scaled_positional_encoding_1 (None, None, 512)         1         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, None, 512)         0         \n",
            "=================================================================\n",
            "Total params: 259,073\n",
            "Trainable params: 259,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQZet88-LNGF"
      },
      "source": [
        "optimizer = WarmUpAdam(p.optimizer_config)\n",
        "checkpointer = Checkpoint(\n",
        "        checkpoint_directory='/content/drive/MyDrive/AI/DATA/Donglinh/ckpt',\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0IlMX6UK5Bk"
      },
      "source": [
        "solver = BaseSolver(\n",
        "        model,\n",
        "        optimizer,\n",
        "        sample_signature=trainset_builder.sample_signature,\n",
        "        eval_sample_signature=testset_builder.sample_signature,\n",
        "        config=p.solver_config,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkKO1ixaLI_S"
      },
      "source": [
        "epoch = int(checkpointer.save_counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nub9N7xK8ky",
        "outputId": "14f49866-4386-4c49-ae4d-7ada5c1fa2fd"
      },
      "source": [
        "while epoch < p.num_epochs:\n",
        "    if rank == 0:\n",
        "        logging.info(\">>>>> start training in epoch %d\" % epoch)\n",
        "    if epoch >= p.sorta_epoch:\n",
        "        trainset_builder.batch_wise_shuffle(p.batch_size)\n",
        "    solver.train(trainset_builder.as_dataset(p.batch_size, p.num_data_threads))\n",
        "\n",
        "    if rank == 0:\n",
        "        logging.info(\">>>>> start evaluate in epoch %d\" % epoch)\n",
        "    devset = devset_builder.as_dataset(p.batch_size, p.num_data_threads)\n",
        "    loss, metrics = solver.evaluate(devset, epoch)\n",
        "\n",
        "    if rank == 0:\n",
        "        checkpointer(loss, metrics)\n",
        "\n",
        "    epoch = epoch + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4WHY71a9Tht",
        "outputId": "849d7e28-9a47-4fcc-b29a-3d2c453bc025"
      },
      "source": [
        "!git clone https://github.com/vlinhd11/athena ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '.'...\n",
            "remote: Enumerating objects: 4271, done.\u001b[K\n",
            "remote: Counting objects: 100% (376/376), done.\u001b[K\n",
            "remote: Compressing objects: 100% (259/259), done.\u001b[K\n",
            "remote: Total 4271 (delta 198), reused 195 (delta 101), pack-reused 3895\u001b[K\n",
            "Receiving objects: 100% (4271/4271), 7.09 MiB | 7.02 MiB/s, done.\n",
            "Resolving deltas: 100% (2704/2704), done.\n",
            "Checking out files: 100% (264/264), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHAPo3et_gh0"
      },
      "source": [
        "!python setup.py bdist_wheel sdist"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}